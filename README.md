# TorsionNet Ramp-Up
Work on extending TorsionNet/AlphaFold will pull together a number of fields
in addition to requiring familiarity with the relevant papers. Here's a compilation 
(in roughly an order that that should serve well for learning the material) of
the relevant material, both for papers and corresponding background skills. The
recommended course is to go through the background tutorials first, before moving
onto the SOTA papers.

## Background Tutorials
### Attention/Transformers
Attention and transformers are used everywhere in sequential data. Given that
amino acid sequences are exactly this, AlphaFold (and likely all other efforts in
the near future) will rely on such modelling techniques, making them worthwhile to
be intimately comfortable with. Here are a set of relevant exercises to work through
along with corresponding reading material:

1. Vanilla RNNs
  - [Exercise](exercises/vanilla.ipynb)
  - [Solution](solutions/vanilla.ipynb)
2. Bidirectional RNNs
  - [Exercise](exercises/bidirectional.ipynb)
  - [Solution](solutions/bidirectional.ipynb)
3. Seq2Seq
  - [Solution](solutions/seq2seq.ipynb)
4. Read https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html and work through Seq2Seq w/ Attention
  - [Solution](solutions/attention.ipynb)
5. Read Lilian's post on [Attention](https://lilianweng.github.io/posts/2018-06-24-attention/)
6. Transformers

### Vanilla RL


### Deep RL


## Papers
### AlphaFold

### TorsionNet
