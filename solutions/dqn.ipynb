{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b21ae2c",
   "metadata": {},
   "source": [
    "# DQN\n",
    "Trying to get DQN to work for MountainCar. Recall that Q learning is an off-policy learning scheme, meaning the TD update (and hence the TD error) is computing by maximizing over the action space. That is, we find:\n",
    "\n",
    "$$ TD = Q(S, A) - (R + \\gamma \\max_A Q(S', A)) $$\n",
    "\n",
    "Where an on-policy would simply have $Q(S', A')$ in place of $\\max_A Q(S', A)$. The two main implementation details that seem to be necessary to get DQNs working are:\n",
    "- Replay buffer: Collect samples in a buffer that gets sampled uniformly at random for training batches. This reduces sample correlation and, therefore, increases training convergence\n",
    "- Separate policy and training networks: The policy network is used to generate samples, which are then used to train the training network. The policy network is updated to the training network after several training steps, which stabilizes the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7eba34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97372a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "882de3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, S, A):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        H = 64\n",
    "        \n",
    "        self.d1 = nn.Linear(S, H)\n",
    "        self.d2 = nn.Linear(H, H)\n",
    "        self.d3 = nn.Linear(H, A)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.d1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.d2(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.d3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ecadac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 / 50000 -- Loss : 1.095138430595398\n",
      "Epoch : 500 / 50000 -- Loss : 0.4307306706905365\n",
      "Epoch : 1000 / 50000 -- Loss : 0.28802597522735596\n",
      "Epoch : 1500 / 50000 -- Loss : 0.36892035603523254\n",
      "Epoch : 2000 / 50000 -- Loss : 0.3032427728176117\n",
      "Epoch : 2500 / 50000 -- Loss : 0.3018014132976532\n",
      "Epoch : 3000 / 50000 -- Loss : 0.31088873744010925\n",
      "Epoch : 3500 / 50000 -- Loss : 0.25752267241477966\n",
      "Successful run!\n",
      "Epoch : 4000 / 50000 -- Loss : 0.28320202231407166\n",
      "Epoch : 4500 / 50000 -- Loss : 0.6292383074760437\n",
      "Epoch : 5000 / 50000 -- Loss : 0.47407397627830505\n",
      "Epoch : 5500 / 50000 -- Loss : 0.7106286883354187\n",
      "Successful run!\n",
      "Epoch : 6000 / 50000 -- Loss : 0.5239637494087219\n",
      "Epoch : 6500 / 50000 -- Loss : 0.5751075148582458\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [142]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(action_probs)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     40\u001b[0m new_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 41\u001b[0m td_target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     42\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     44\u001b[0m train_sample \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39marray(state), action, td_target\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bliss/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [141]\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(output)\n\u001b[1;32m     16\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md2(output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bliss/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bliss/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 50_000\n",
    "epsilon = 1.0\n",
    "gamma = .95\n",
    "lr = 0.0001\n",
    "\n",
    "batch_size = 100\n",
    "max_memory = 10_000\n",
    "    \n",
    "S = env.observation_space.shape[0]\n",
    "A = env.action_space.n\n",
    "\n",
    "policy_net = DQN(S, A)\n",
    "\n",
    "loss_criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "replay_buffer = []\n",
    "\n",
    "losses = []\n",
    "total_rewards = []\n",
    "max_total_reward = -200\n",
    "\n",
    "for episode in range(episodes):\n",
    "    if episode % 2_000 == 0 and max_total_reward > 100:\n",
    "        print(\"Successful run!\")\n",
    "        epsilon /= 2\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action_probs = policy_net(torch.tensor(state))\n",
    "            action = torch.argmax(action_probs).detach().numpy()\n",
    "            \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        td_target = reward + gamma * torch.max(policy_net(torch.tensor(new_state)))\n",
    "        total_reward += reward\n",
    "        \n",
    "        train_sample = (np.array(state), action, td_target.detach().numpy())\n",
    "        replay_buffer.append(train_sample)\n",
    "        \n",
    "        if len(replay_buffer) > max_memory:\n",
    "            replay_buffer.pop(0)\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        # ================== #\n",
    "        batch = [random.choice(replay_buffer) for _ in range(batch_size)]\n",
    "        batch_states, batch_actions, batch_targets = list(zip(*batch))\n",
    "\n",
    "        batch_states  = torch.tensor(np.array(batch_states))\n",
    "        batch_actions = torch.tensor(np.array(batch_actions))\n",
    "        batch_targets = torch.tensor(np.array(batch_targets))\n",
    "\n",
    "        predicted_qs = policy_net(batch_states)\n",
    "        batch_indices = np.arange(batch_size, dtype=np.int64)\n",
    "        predicted_targets = predicted_qs.gather(1, batch_actions.view(-1,1)).flatten() \n",
    "        \n",
    "        loss = loss_criterion(predicted_targets, batch_targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.detach().numpy())\n",
    "    \n",
    "#     print(total_reward)\n",
    "        \n",
    "#     if max_total_reward > -200:\n",
    "#         print(\"Successful run!\")\n",
    "    \n",
    "    max_total_reward = max(total_reward, max_total_reward)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    if episode % 500 == 0:\n",
    "        print(f\"Epoch : {episode} / {episodes} -- Loss : {loss} -- Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(total_rewards)), total_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bba0f8fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ce6237f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.960464477539063e-08"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "10b5ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render(mode='rgb_array')\n",
    "    action_probs = policy_net(torch.tensor(state))\n",
    "    action = torch.argmax(action_probs).detach().numpy()\n",
    "    state, _, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e9a3d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4j/yb1phr856mgf1c7fysrq3_6c0000gs/T/ipykernel_36517/2878368158.py:62: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Average Reward 45.0 Best Reward 45.0 Last Reward 45.0 Epsilon 1.0\n",
      "Episode 2 Average Reward 29.0 Best Reward 45.0 Last Reward 13.0 Epsilon 1.0\n",
      "Episode 3 Average Reward 23.333333333333332 Best Reward 45.0 Last Reward 12.0 Epsilon 0.993020965034979\n",
      "Episode 4 Average Reward 21.25 Best Reward 45.0 Last Reward 15.0 Epsilon 0.9782294672887405\n",
      "Episode 5 Average Reward 24.0 Best Reward 45.0 Last Reward 35.0 Epsilon 0.9445671308589195\n",
      "Episode 6 Average Reward 22.666666666666668 Best Reward 45.0 Last Reward 16.0 Epsilon 0.9295668775782806\n",
      "Episode 7 Average Reward 23.428571428571427 Best Reward 45.0 Last Reward 28.0 Epsilon 0.9038873549665959\n",
      "Episode 8 Average Reward 22.875 Best Reward 45.0 Last Reward 19.0 Epsilon 0.8868671875860644\n",
      "Episode 9 Average Reward 22.333333333333332 Best Reward 45.0 Last Reward 18.0 Epsilon 0.8710385479118223\n",
      "Episode 10 Average Reward 21.2 Best Reward 45.0 Last Reward 11.0 Epsilon 0.8615048875706075\n",
      "Episode 11 Average Reward 20.818181818181817 Best Reward 45.0 Last Reward 17.0 Epsilon 0.8469758853683546\n",
      "Episode 12 Average Reward 20.0 Best Reward 45.0 Last Reward 11.0 Epsilon 0.8377055948310879\n",
      "Episode 13 Average Reward 20.307692307692307 Best Reward 45.0 Last Reward 24.0 Epsilon 0.8178301806491574\n",
      "Episode 14 Average Reward 21.571428571428573 Best Reward 45.0 Last Reward 38.0 Epsilon 0.7873207291459607\n",
      "Episode 15 Average Reward 21.133333333333333 Best Reward 45.0 Last Reward 15.0 Epsilon 0.7755932297267324\n",
      "Episode 16 Average Reward 21.0625 Best Reward 45.0 Last Reward 20.0 Epsilon 0.7602278474153183\n",
      "Episode 17 Average Reward 21.176470588235293 Best Reward 45.0 Last Reward 23.0 Epsilon 0.7429336049129575\n",
      "Episode 18 Average Reward 20.666666666666668 Best Reward 45.0 Last Reward 12.0 Epsilon 0.7340672721936974\n",
      "Episode 19 Average Reward 20.42105263157895 Best Reward 45.0 Last Reward 16.0 Epsilon 0.7224098741663908\n",
      "Episode 20 Average Reward 20.8 Best Reward 45.0 Last Reward 28.0 Epsilon 0.7024531167280339\n",
      "Episode 21 Average Reward 21.142857142857142 Best Reward 45.0 Last Reward 28.0 Epsilon 0.6830476698153162\n",
      "Episode 22 Average Reward 21.454545454545453 Best Reward 45.0 Last Reward 28.0 Epsilon 0.6641783033340387\n",
      "Episode 23 Average Reward 21.52173913043478 Best Reward 45.0 Last Reward 23.0 Epsilon 0.649069069067341\n",
      "Episode 24 Average Reward 21.041666666666668 Best Reward 45.0 Last Reward 10.0 Epsilon 0.6426075087326283\n",
      "Episode 25 Average Reward 21.2 Best Reward 45.0 Last Reward 25.0 Epsilon 0.6267336333646188\n",
      "Episode 26 Average Reward 21.0 Best Reward 45.0 Last Reward 16.0 Epsilon 0.6167807534338766\n",
      "Episode 27 Average Reward 21.074074074074073 Best Reward 45.0 Last Reward 23.0 Epsilon 0.6027497547577038\n",
      "Episode 28 Average Reward 21.178571428571427 Best Reward 45.0 Last Reward 24.0 Epsilon 0.5884489059896089\n",
      "Episode 29 Average Reward 21.75862068965517 Best Reward 45.0 Last Reward 38.0 Epsilon 0.5664966061305491\n",
      "Episode 30 Average Reward 21.766666666666666 Best Reward 45.0 Last Reward 22.0 Epsilon 0.5541636732359665\n",
      "Episode 31 Average Reward 22.06451612903226 Best Reward 45.0 Last Reward 31.0 Epsilon 0.5372398118510032\n",
      "Episode 32 Average Reward 22.03125 Best Reward 45.0 Last Reward 21.0 Epsilon 0.5260698848381481\n",
      "Episode 33 Average Reward 21.78787878787879 Best Reward 45.0 Last Reward 14.0 Epsilon 0.5187525878460405\n",
      "Episode 34 Average Reward 22.147058823529413 Best Reward 45.0 Last Reward 34.0 Epsilon 0.5014029397595902\n",
      "Episode 35 Average Reward 22.8 Best Reward 45.0 Last Reward 45.0 Epsilon 0.4793291555710539\n",
      "Episode 36 Average Reward 23.444444444444443 Best Reward 46.0 Last Reward 46.0 Epsilon 0.45776892144096987\n",
      "Episode 37 Average Reward 23.62162162162162 Best Reward 46.0 Last Reward 30.0 Epsilon 0.44423313721693997\n",
      "Episode 38 Average Reward 24.289473684210527 Best Reward 49.0 Last Reward 49.0 Epsilon 0.4229800403927701\n",
      "Episode 39 Average Reward 25.128205128205128 Best Reward 57.0 Last Reward 57.0 Epsilon 0.3995330431643887\n",
      "Episode 40 Average Reward 25.75 Best Reward 57.0 Last Reward 50.0 Epsilon 0.380038079308654\n",
      "Episode 41 Average Reward 26.26829268292683 Best Reward 57.0 Last Reward 47.0 Epsilon 0.36258101563401773\n",
      "Episode 42 Average Reward 26.0 Best Reward 57.0 Last Reward 15.0 Epsilon 0.3571802069256231\n",
      "Episode 43 Average Reward 25.767441860465116 Best Reward 57.0 Last Reward 16.0 Epsilon 0.35150798586723914\n",
      "Episode 44 Average Reward 26.704545454545453 Best Reward 67.0 Last Reward 67.0 Epsilon 0.3287175620446999\n",
      "Episode 45 Average Reward 27.666666666666668 Best Reward 70.0 Last Reward 70.0 Epsilon 0.3064834890782873\n",
      "Episode 46 Average Reward 27.5 Best Reward 70.0 Last Reward 20.0 Epsilon 0.30041170324864147\n",
      "Episode 47 Average Reward 27.319148936170212 Best Reward 70.0 Last Reward 19.0 Epsilon 0.2947549613501431\n",
      "Episode 48 Average Reward 27.25 Best Reward 70.0 Last Reward 24.0 Epsilon 0.28776160118260813\n",
      "Episode 49 Average Reward 28.306122448979593 Best Reward 79.0 Last Reward 79.0 Epsilon 0.2658926982385883\n",
      "Episode 50 Average Reward 28.44 Best Reward 79.0 Last Reward 35.0 Epsilon 0.25674293352423394\n",
      "Episode 51 Average Reward 29.215686274509803 Best Reward 79.0 Last Reward 68.0 Epsilon 0.23985661395742483\n",
      "Episode 52 Average Reward 29.53846153846154 Best Reward 79.0 Last Reward 46.0 Epsilon 0.2290678591018803\n",
      "Episode 53 Average Reward 29.735849056603772 Best Reward 79.0 Last Reward 40.0 Epsilon 0.22008157526211472\n",
      "Episode 54 Average Reward 30.51851851851852 Best Reward 79.0 Last Reward 72.0 Epsilon 0.20478532805697028\n",
      "Episode 55 Average Reward 31.89090909090909 Best Reward 106.0 Last Reward 106.0 Epsilon 0.18417920334939616\n",
      "Episode 56 Average Reward 31.892857142857142 Best Reward 106.0 Last Reward 32.0 Epsilon 0.17837591478438933\n",
      "Episode 57 Average Reward 32.63157894736842 Best Reward 106.0 Last Reward 74.0 Epsilon 0.16564652979915298\n",
      "Episode 58 Average Reward 33.55172413793103 Best Reward 106.0 Last Reward 86.0 Epsilon 0.15198976010464077\n",
      "Episode 59 Average Reward 33.813559322033896 Best Reward 106.0 Last Reward 49.0 Epsilon 0.1447182334733242\n",
      "Episode 60 Average Reward 34.916666666666664 Best Reward 106.0 Last Reward 100.0 Epsilon 0.13093992119083192\n",
      "Episode 61 Average Reward 36.18032786885246 Best Reward 112.0 Last Reward 112.0 Epsilon 0.11705952472752763\n",
      "Episode 62 Average Reward 37.854838709677416 Best Reward 140.0 Last Reward 140.0 Epsilon 0.1017595336843962\n",
      "Episode 63 Average Reward 38.95238095238095 Best Reward 140.0 Last Reward 107.0 Epsilon 0.09142865865928752\n",
      "Episode 64 Average Reward 40.375 Best Reward 140.0 Last Reward 130.0 Epsilon 0.08027786571150902\n",
      "Episode 65 Average Reward 41.6 Best Reward 140.0 Last Reward 120.0 Epsilon 0.07119580498897772\n",
      "Episode 66 Average Reward 42.333333333333336 Best Reward 140.0 Last Reward 90.0 Epsilon 0.06506513648938707\n",
      "Episode 67 Average Reward 43.417910447761194 Best Reward 140.0 Last Reward 115.0 Epsilon 0.05799352286699774\n",
      "Episode 68 Average Reward 45.6764705882353 Best Reward 197.0 Last Reward 197.0 Epsilon 0.047619043934701566\n",
      "Episode 69 Average Reward 47.05797101449275 Best Reward 197.0 Last Reward 141.0 Epsilon 0.04135371320577681\n",
      "Episode 70 Average Reward 49.285714285714285 Best Reward 203.0 Last Reward 203.0 Epsilon 0.03375270793242542\n",
      "Episode 71 Average Reward 50.87323943661972 Best Reward 203.0 Last Reward 162.0 Epsilon 0.028702367053437684\n",
      "Episode 72 Average Reward 52.5 Best Reward 203.0 Last Reward 168.0 Epsilon 0.024261616631971107\n",
      "Episode 73 Average Reward 54.178082191780824 Best Reward 203.0 Last Reward 175.0 Epsilon 0.020364801229907723\n",
      "Episode 74 Average Reward 55.108108108108105 Best Reward 203.0 Last Reward 123.0 Epsilon 0.01800674553510103\n",
      "Episode 75 Average Reward 57.38666666666666 Best Reward 226.0 Last Reward 226.0 Epsilon 0.014362682707666368\n",
      "Episode 76 Average Reward 58.421052631578945 Best Reward 226.0 Last Reward 136.0 Epsilon 0.012535508771838856\n",
      "Episode 77 Average Reward 59.77922077922078 Best Reward 226.0 Last Reward 163.0 Epsilon 0.010649189271466766\n",
      "Episode 78 Average Reward 61.03846153846154 Best Reward 226.0 Last Reward 158.0 Epsilon 0.00909208915491982\n",
      "Episode 79 Average Reward 62.392405063291136 Best Reward 226.0 Last Reward 168.0 Epsilon 0.0076853864020928085\n",
      "Episode 80 Average Reward 63.175 Best Reward 226.0 Last Reward 125.0 Epsilon 0.006781905529861685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 81 Average Reward 64.25925925925925 Best Reward 226.0 Last Reward 151.0 Epsilon 0.005830965314140695\n",
      "Episode 82 Average Reward 65.48780487804878 Best Reward 226.0 Last Reward 165.0 Epsilon 0.004943630640136612\n",
      "Episode 83 Average Reward 66.62650602409639 Best Reward 226.0 Last Reward 160.0 Epsilon 0.004212346918856964\n",
      "Episode 84 Average Reward 67.17857142857143 Best Reward 226.0 Last Reward 113.0 Epsilon 0.00376204772769516\n",
      "Episode 85 Average Reward 69.2 Best Reward 239.0 Last Reward 239.0 Epsilon 0.0029619381600508587\n",
      "Episode 86 Average Reward 69.52325581395348 Best Reward 239.0 Last Reward 97.0 Epsilon 0.0026879943090827602\n",
      "Episode 87 Average Reward 71.01149425287356 Best Reward 239.0 Last Reward 199.0 Epsilon 0.0022027261208967326\n",
      "Episode 88 Average Reward 72.9090909090909 Best Reward 239.0 Last Reward 238.0 Epsilon 0.0017359879235368028\n",
      "Episode 89 Average Reward 73.47191011235955 Best Reward 239.0 Last Reward 123.0 Epsilon 0.0015349765724807553\n",
      "Episode 90 Average Reward 74.85555555555555 Best Reward 239.0 Last Reward 198.0 Epsilon 0.0012591237627402137\n",
      "Episode 91 Average Reward 75.28571428571429 Best Reward 239.0 Last Reward 114.0 Epsilon 0.00112339911644727\n",
      "Episode 92 Average Reward 76.31521739130434 Best Reward 239.0 Last Reward 170.0 Epsilon 0.001\n",
      "Episode 93 Average Reward 76.74193548387096 Best Reward 239.0 Last Reward 116.0 Epsilon 0.001\n",
      "Episode 94 Average Reward 78.1063829787234 Best Reward 239.0 Last Reward 205.0 Epsilon 0.001\n",
      "Episode 95 Average Reward 79.77894736842106 Best Reward 239.0 Last Reward 237.0 Epsilon 0.001\n",
      "Episode 96 Average Reward 81.19791666666667 Best Reward 239.0 Last Reward 216.0 Epsilon 0.001\n",
      "Episode 97 Average Reward 82.37113402061856 Best Reward 239.0 Last Reward 195.0 Epsilon 0.001\n",
      "Episode 98 Average Reward 83.0204081632653 Best Reward 239.0 Last Reward 146.0 Epsilon 0.001\n",
      "Episode 99 Average Reward 84.28282828282828 Best Reward 239.0 Last Reward 208.0 Epsilon 0.001\n",
      "Episode 100 Average Reward 85.47 Best Reward 239.0 Last Reward 203.0 Epsilon 0.001\n",
      "Episode 101 Average Reward 86.97029702970298 Best Reward 239.0 Last Reward 237.0 Epsilon 0.001\n",
      "Episode 102 Average Reward 88.51960784313725 Best Reward 245.0 Last Reward 245.0 Epsilon 0.001\n",
      "Episode 103 Average Reward 89.52427184466019 Best Reward 245.0 Last Reward 192.0 Epsilon 0.001\n",
      "Episode 104 Average Reward 90.82692307692308 Best Reward 245.0 Last Reward 225.0 Epsilon 0.001\n",
      "Episode 105 Average Reward 91.77142857142857 Best Reward 245.0 Last Reward 190.0 Epsilon 0.001\n",
      "Episode 106 Average Reward 94.44339622641509 Best Reward 375.0 Last Reward 375.0 Epsilon 0.001\n",
      "Episode 107 Average Reward 96.13084112149532 Best Reward 375.0 Last Reward 275.0 Epsilon 0.001\n",
      "Episode 108 Average Reward 96.51851851851852 Best Reward 375.0 Last Reward 138.0 Epsilon 0.001\n",
      "Episode 109 Average Reward 96.90825688073394 Best Reward 375.0 Last Reward 139.0 Epsilon 0.001\n",
      "Episode 110 Average Reward 97.2909090909091 Best Reward 375.0 Last Reward 139.0 Epsilon 0.001\n",
      "Episode 111 Average Reward 98.63963963963964 Best Reward 375.0 Last Reward 247.0 Epsilon 0.001\n",
      "Episode 112 Average Reward 99.32142857142857 Best Reward 375.0 Last Reward 175.0 Epsilon 0.001\n",
      "Episode 113 Average Reward 100.02654867256638 Best Reward 375.0 Last Reward 179.0 Epsilon 0.001\n",
      "Episode 114 Average Reward 100.94736842105263 Best Reward 375.0 Last Reward 205.0 Epsilon 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [138]\u001b[0m, in \u001b[0;36m<cell line: 142>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m state_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state_, [\u001b[38;5;241m1\u001b[39m, observation_space])\n\u001b[1;32m    152\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39madd(state, action, reward, state_, done)\n\u001b[0;32m--> 153\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m state \u001b[38;5;241m=\u001b[39m state_\n\u001b[1;32m    155\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Input \u001b[0;32mIn [138]\u001b[0m, in \u001b[0;36mDQN_Solver.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m         next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(states_)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#         predicted_value_of_now = torch.index_select(q_values, 1, actions) \u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#         print(predicted_value_of_now.shape)\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m         predicted_value_of_now \u001b[38;5;241m=\u001b[39m \u001b[43mq_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#         print(predicted_value_of_now.shape)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#         print(\"=================================\")\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         predicted_value_of_future \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(next_q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "EPISODES = 1000\n",
    "LEARNING_RATE = 0.0001\n",
    "MEM_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.999\n",
    "EXPLORATION_MIN = 0.001\n",
    "\n",
    "FC1_DIMS = 1024\n",
    "FC2_DIMS = 512\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = env.observation_space.shape\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.input_shape, FC1_DIMS)\n",
    "        self.fc2 = nn.Linear(FC1_DIMS, FC2_DIMS)\n",
    "        self.fc3 = nn.Linear(FC2_DIMS, self.action_space)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.mem_count = 0\n",
    "        \n",
    "        self.states = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
    "        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n",
    "        self.states_ = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n",
    "    \n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        mem_index = self.mem_count % MEM_SIZE\n",
    "        \n",
    "        self.states[mem_index]  = state\n",
    "        self.actions[mem_index] = action\n",
    "        self.rewards[mem_index] = reward\n",
    "        self.states_[mem_index] = state_\n",
    "        self.dones[mem_index] =  1 - done\n",
    "\n",
    "        self.mem_count += 1\n",
    "    \n",
    "    def sample(self):\n",
    "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
    "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n",
    "        \n",
    "        states  = self.states[batch_indices]\n",
    "        actions = self.actions[batch_indices]\n",
    "        rewards = self.rewards[batch_indices]\n",
    "        states_ = self.states_[batch_indices]\n",
    "        dones   = self.dones[batch_indices]\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "class DQN_Solver:\n",
    "    def __init__(self):\n",
    "        self.memory = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return env.action_space.sample()\n",
    "        \n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.to(DEVICE)\n",
    "        state = state.unsqueeze(0)\n",
    "        q_values = self.network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_count < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, states_, dones = self.memory.sample()\n",
    "        states = torch.tensor(states , dtype=torch.float32).to(DEVICE)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(DEVICE)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
    "        states_ = torch.tensor(states_, dtype=torch.float32).to(DEVICE)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(DEVICE)\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        q_values = self.network(states)\n",
    "        next_q_values = self.network(states_)\n",
    "        \n",
    "#         predicted_value_of_now = torch.index_select(q_values, 1, actions) \n",
    "#         print(predicted_value_of_now.shape)\n",
    "        \n",
    "        predicted_value_of_now = q_values[batch_indices, actions]\n",
    "#         print(predicted_value_of_now.shape)\n",
    "#         print(\"=================================\")\n",
    "        \n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
    "        \n",
    "        q_target = rewards + GAMMA * predicted_value_of_future * dones\n",
    "        \n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate\n",
    "\n",
    "agent = DQN_Solver()\n",
    "\n",
    "for i in range(1, EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.reshape(state_, [1, observation_space])\n",
    "        agent.memory.add(state, action, reward, state_, done)\n",
    "        agent.learn()\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "            break\n",
    "            \n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)\n",
    "\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c5c17dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584752e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
